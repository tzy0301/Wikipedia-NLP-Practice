# Wikipedia-NLP-Practice
Using Databricks and Apache Spark (PySpark) to perform distributed processing on the Wikipedia corpus (54MB, 400,000+ entries), combined with the MAGPIE idiom corpus for NLP and semantic structure analysis.
